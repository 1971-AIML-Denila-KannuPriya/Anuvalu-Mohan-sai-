{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb747c46-b2c2-4cd1-8238-3330cb6f8873",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f80b2103-1246-4c5a-9f7d-51681285f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5575 - loss: 0.6807  \n",
      "Model accuracy with Learning Rate Reduction: 57.00%\n"
     ]
    }
   ],
   "source": [
    "#19. Using Callbacks to Reduce Learning Rate on Plateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(500, 10)\n",
    "y = np.random.randint(2, size=500)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "lr_reduction = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "\n",
    "model.fit(X, y, epochs=30, callbacks=[lr_reduction], verbose=0)\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f'Model accuracy with Learning Rate Reduction: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf8ac0-f837-4224-80e2-3a3a52f7c1ff",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d39b855-2c0b-49d3-9de1-835d2fefb7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained input-to-hidden weights: [[3.66766056 5.79399811]\n",
      " [3.69075066 5.91518668]]\n",
      "Trained hidden-to-output weights: [[-8.13983112]\n",
      " [ 7.48568666]]\n"
     ]
    }
   ],
   "source": [
    "#3. Multilayer Perceptron (MLP) for XOR Problem\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "weights_input_hidden = np.random.rand(2, 2)  \n",
    "weights_hidden_output = np.random.rand(2, 1)  \n",
    "\n",
    "bias_hidden = np.random.rand(1, 2)\n",
    "bias_output = np.random.rand(1, 1)\n",
    "\n",
    "\n",
    "def train(X, y, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output, epochs=10000, lr=0.1):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "        hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "        \n",
    "        output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "        predicted_output = sigmoid(output_layer_input)\n",
    "        \n",
    "        \n",
    "        error = y - predicted_output\n",
    "        \n",
    "        \n",
    "        d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "        \n",
    "        error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
    "        d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "        \n",
    "        \n",
    "        weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
    "        weights_input_hidden += X.T.dot(d_hidden_layer) * lr\n",
    "        bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * lr\n",
    "        bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * lr\n",
    "    \n",
    "    print(\"Trained input-to-hidden weights:\", weights_input_hidden)\n",
    "    print(\"Trained hidden-to-output weights:\", weights_hidden_output)\n",
    "\n",
    "\n",
    "train(X, y, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe01b2-b304-4350-91d9-51554278df66",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea656755-023e-4f7d-bee7-81f417f435af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Using Keras to Build a Simple Neural Network for Binary Classification\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=2, input_dim=2, activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=10000, verbose=0)\n",
    "\n",
    "accuracy = model.evaluate(X, y)\n",
    "print(f'Model accuracy: {accuracy[1]*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1b7ba-8344-4c47-ab2b-2ef757f6e1d5",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbdf021-a0a9-4dcf-858e-ec5751369824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. CNN for Image Classification with TensorFlow (MNIST dataset)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c969dae-c558-41c7-b54c-d229d289b94c",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2962c93-0c97-4f1b-97ad-d040c6e41e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Using Dropout for Regularization in Neural Networks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=16, input_dim=2, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=5000, verbose=0)\n",
    "\n",
    "accuracy = model.evaluate(X, y)\n",
    "print(f'Model accuracy with dropout: {accuracy[1]*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84adcfbc-08d0-4409-bdb1-1c16d4432e8d",
   "metadata": {},
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8218ef-216c-41ca-bf5e-e30bd25faae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Multiclass Classification with Keras (Iris Dataset)\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = to_categorical(iris.target, 3)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=4, activation='relu'))  \n",
    "model.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, verbose=0)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Model accuracy on Iris Dataset: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8904b40-8168-4739-a204-9b4d9c7e43b1",
   "metadata": {},
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433300f-0684-4605-8d9a-2607183baa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Using LSTM for Time Series Prediction\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "time_steps = 10\n",
    "X = np.random.rand(100, time_steps, 1)  \n",
    "y = np.random.rand(100, 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(time_steps, 1))) \n",
    "model.add(Dense(1))  \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit(X, y, epochs=10, verbose=0)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "print(f'Sample prediction: {predictions[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16faa1e-4c10-4b0a-a5d4-857fd771ed22",
   "metadata": {},
   "source": [
    "# 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde4b6f-0726-4e02-a050-ced800979a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Image Classification with Data Augmentation (CIFAR-10 dataset)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(train_images)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(datagen.flow(train_images, train_labels, batch_size=32), epochs=5)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy with data augmentation: {test_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a33d5-e148-4f02-b18f-1950a95d6c23",
   "metadata": {},
   "source": [
    "# 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122932d-ccf4-44c9-b49c-79973cc8686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Autoencoder for Dimensionality Reduction\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "X = np.random.rand(1000, 20)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "\n",
    "autoencoder.add(Dense(10, input_dim=20, activation='relu'))  \n",
    "autoencoder.add(Dense(5, activation='relu'))  \n",
    "\n",
    "autoencoder.add(Dense(10, activation='relu'))\n",
    "autoencoder.add(Dense(20, activation='sigmoid'))  \n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "autoencoder.fit(X, X, epochs=10, verbose=0)\n",
    "\n",
    "encoder = Sequential(autoencoder.layers[:2])\n",
    "encoded_data = encoder.predict(X)\n",
    "print(f'Original data shape: {X.shape}, Encoded data shape: {encoded_data.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27df118-56ea-430b-b5e3-06b1741347a0",
   "metadata": {},
   "source": [
    "# 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0bf43-0797-4a82-8a0d-9bac9a1c0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Transfer Learning Using Pretrained VGG16 Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  \n",
    "])\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "train_images = np.random.rand(100, 224, 224, 3)\n",
    "train_labels = np.random.randint(0, 10, 100)\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "test_images = np.random.rand(10, 224, 224, 3)\n",
    "test_labels = np.random.randint(0, 10, 10)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy with transfer learning: {test_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4567517-2e6a-46f7-a82a-092166c5c7de",
   "metadata": {},
   "source": [
    "# 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a3cec-1dfe-467e-8d53-19e16aa161a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. Recurrent Neural Network (RNN) for Sequence Classification\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "X = np.random.rand(100, 10, 1)\n",
    "y = np.random.randint(2, size=100)  \n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e564b-3b19-4b79-81e2-3a9717dd438c",
   "metadata": {},
   "source": [
    "# 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ecbe54-efa4-4017-92dc-1e91f3345862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13. Implementing Batch Normalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(1000, 20)  \n",
    "y = np.random.randint(2, size=1000)  \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(BatchNormalization())  \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=50, verbose=0)\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f'Model accuracy with Batch Normalization: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9a7d9-411b-40ae-8aa9-146f2a754e86",
   "metadata": {},
   "source": [
    "# 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4750a-2925-427a-8b04-b9754c905720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14. Implementing Early Stopping Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.random.rand(500, 10)\n",
    "y = np.random.randint(2, size=500)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X, y, validation_split=0.2, epochs=100, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f'Model accuracy with Early Stopping: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1cae3-8df3-4ab8-bb62-7f168ec24b1f",
   "metadata": {},
   "source": [
    "# 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09050af6-240a-439a-a572-617c636eeaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15. Using L1 and L2 Regularization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(500, 10)\n",
    "y = np.random.randint(2, size=500)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=10, activation='relu',\n",
    "                kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=10, verbose=0)\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f'Model accuracy with L1 and L2 Regularization: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2ec98-0609-4af1-9f43-728197c06f07",
   "metadata": {},
   "source": [
    "# 15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41fefd-6e3d-445f-833c-cb36783992c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import get_custom_objects\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "def swish(x):\n",
    "    return x * K.sigmoid(x)\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "X = np.random.rand(500, 10)\n",
    "y = np.random.randint(2, size=500)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=10))\n",
    "model.add(Activation('swish'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=30, verbose=0)\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f'Model accuracy with Custom Activation Function: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02c6c8-a41f-4645-a2de-6042eb62f107",
   "metadata": {},
   "source": [
    "# 16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46740b04-2a53-4235-bbd3-03d52f7fb123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Multiply, Activation, Flatten\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(100, 10, 1)\n",
    "y = np.random.randint(2, size=100)\n",
    "\n",
    "inputs = Input(shape=(10, 1))\n",
    "lstm_out = LSTM(32, return_sequences=True)(inputs)\n",
    "attention = Dense(1, activation='tanh')(lstm_out)\n",
    "attention = Activation('softmax')(attention)\n",
    "context = Multiply()([lstm_out, attention])\n",
    "context_vector = Flatten()(context)\n",
    "output = Dense(1, activation='sigmoid')(context_vector)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=10, verbose=0)\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f'Model accuracy with Attention Mechanism: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e4f67-8db1-4511-b7e0-5d25714bbcd4",
   "metadata": {},
   "source": [
    "# 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07105a08-5312-4523-b88c-2067ca0c3c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18. Implementing K-Fold Cross-Validation\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(500, 10)\n",
    "y = np.random.randint(2, size=500)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[test_index]\n",
    "    y_train, y_val = y[train_index], y[test_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    scores.append(accuracy * 100)\n",
    "    \n",
    "print(f'Cross-Validation Accuracy Scores: {scores}')\n",
    "print(f'Mean Accuracy: {np.mean(scores):.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358f2b9-a5c7-4d03-bbea-26ad4709331b",
   "metadata": {},
   "source": [
    "# 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571d466-01e6-446f-8f96-221cd2d5b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Single-Layer Neural Network with Sigmoid Activation\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "outputs = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "weights = np.random.rand(2, 1)\n",
    "bias = np.random.rand(1)\n",
    "\n",
    "def train(inputs, outputs, weights, bias, epochs=10000, lr=0.1):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        weighted_sum = np.dot(inputs, weights) + bias\n",
    "        predicted_output = sigmoid(weighted_sum)\n",
    "        \n",
    "        error = outputs - predicted_output\n",
    "        \n",
    "        \n",
    "        adjustments = error * sigmoid_derivative(predicted_output)\n",
    "        weights += np.dot(inputs.T, adjustments) * lr\n",
    "        bias += np.sum(adjustments) * lr\n",
    "    \n",
    "    print(\"Trained weights:\", weights)\n",
    "    print(\"Trained bias:\", bias)\n",
    "\n",
    "\n",
    "train(inputs, outputs, weights, bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc420821-bea7-479a-bb24-f31a10a58025",
   "metadata": {},
   "source": [
    "# 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded93298-5e8c-4b3d-9745-54fb15e2b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Simple Perceptron for Binary Classification\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "weights = np.random.rand(2)\n",
    "bias = np.random.rand(1)\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "    \n",
    "def perceptron(X, y, weights, bias, epochs=10, lr=0.1):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            weighted_sum = np.dot(X[i], weights) + bias\n",
    "            prediction = step_function(weighted_sum)\n",
    "            \n",
    "            error = y[i] - prediction\n",
    "            weights += lr * error * X[i]\n",
    "            bias += lr * error\n",
    "        print(f'Epoch {epoch+1}, Weights: {weights}, Bias: {bias}')\n",
    "        \n",
    "\n",
    "perceptron(X, y, weights, bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e6e98-855b-4d2d-a3de-376adddfa21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97f4ce-49c2-422a-add9-1b7af7d725f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(500, 10)\n",
    "y = np.random.randint(2, size=500)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, y, validation_split=0.2, epochs=10, verbose=0)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e18b4-eae9-44e9-ae65-c829a2d9323e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
